{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AE설명 : http://solarisailab.com/archives/113\n",
    "- VAE : http://nolsigan.com/blog/what-is-variational-autoencoder/\n",
    "- Auto-Encoding Variational Bayes : https://arxiv.org/abs/1312.6114  [[code](https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW)]\n",
    "- Tutorial on Variational Autoencoders : https://arxiv.org/abs/1606.05908\n",
    "- GAN : http://aliensunmin.github.io/project/accv16tutorial/media/generative.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-04-13 03:14:16--  http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Resolving yann.lecun.com... 216.165.22.6\n",
      "Connecting to yann.lecun.com|216.165.22.6|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9912422 (9.5M) [application/x-gzip]\n",
      "Saving to: 'data/train-images-idx3-ubyte.gz'\n",
      "\n",
      "train-images-idx3-u 100%[=====================>]   9.45M  3.53MB/s   in 2.7s   \n",
      "\n",
      "2017-04-13 03:14:19 (3.53 MB/s) - 'data/train-images-idx3-ubyte.gz' saved [9912422/9912422]\n",
      "\n",
      "--2017-04-13 03:14:19--  http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Resolving yann.lecun.com... 216.165.22.6\n",
      "Connecting to yann.lecun.com|216.165.22.6|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28881 (28K) [application/x-gzip]\n",
      "Saving to: 'data/train-labels-idx1-ubyte.gz'\n",
      "\n",
      "train-labels-idx1-u 100%[=====================>]  28.20K  92.0KB/s   in 0.3s   \n",
      "\n",
      "2017-04-13 03:14:20 (92.0 KB/s) - 'data/train-labels-idx1-ubyte.gz' saved [28881/28881]\n",
      "\n",
      "--2017-04-13 03:14:20--  http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Resolving yann.lecun.com... 216.165.22.6\n",
      "Connecting to yann.lecun.com|216.165.22.6|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1648877 (1.6M) [application/x-gzip]\n",
      "Saving to: 'data/t10k-images-idx3-ubyte.gz'\n",
      "\n",
      "t10k-images-idx3-ub 100%[=====================>]   1.57M  3.56MB/s   in 0.4s   \n",
      "\n",
      "2017-04-13 03:14:21 (3.56 MB/s) - 'data/t10k-images-idx3-ubyte.gz' saved [1648877/1648877]\n",
      "\n",
      "--2017-04-13 03:14:21--  http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Resolving yann.lecun.com... 216.165.22.6\n",
      "Connecting to yann.lecun.com|216.165.22.6|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4542 (4.4K) [application/x-gzip]\n",
      "Saving to: 'data/t10k-labels-idx1-ubyte.gz'\n",
      "\n",
      "t10k-labels-idx1-ub 100%[=====================>]   4.44K  --.-KB/s   in 0s     \n",
      "\n",
      "2017-04-13 03:14:22 (108 MB/s) - 'data/t10k-labels-idx1-ubyte.gz' saved [4542/4542]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download mnist here: http://yann.lecun.com/exdb/mnist/\n",
    "!wget -P data/ -c http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!wget -P data/ -c http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "!wget -P data/ -c http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "!wget -P data/ -c http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  \n",
    "# unzip\n",
    "!gzip -d data/train-images-idx3-ubyte.gz\n",
    "!gzip -d data/train-labels-idx1-ubyte.gz\n",
    "!gzip -d data/t10k-images-idx3-ubyte.gz\n",
    "!gzip -d data/t10k-labels-idx1-ubyte.gz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    with open('data/train-images-idx3-ubyte', 'rb') as f:\n",
    "        data = np.fromfile(file=f, dtype=np.uint8)\n",
    "    X_train = data[16:].reshape(60000, 28 * 28).astype(np.float32)\n",
    "    with open('data/train-labels-idx1-ubyte', 'rb') as f:\n",
    "        data = np.fromfile(file=f, dtype=np.uint8)\n",
    "    y_train = data[8:].reshape(60000).astype(np.uint8)\n",
    "\n",
    "    with open('data/t10k-images-idx3-ubyte', 'rb') as f:\n",
    "        data = np.fromfile(file=f, dtype=np.uint8)\n",
    "    X_test = data[16:].reshape(10000, 28 * 28).astype(np.float32)\n",
    "    with open('data/t10k-labels-idx1-ubyte', 'rb') as f:\n",
    "        data = np.fromfile(file=f, dtype=np.uint8)\n",
    "    y_test = data[8:].reshape(10000).astype(np.uint8)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "    X_train /= 255.\n",
    "    X_test /= 255.\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n",
      "0.0 0.0\n",
      "0.130661 0.132515\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, _, X_test, _ = load_mnist()\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(X_train.min(), X_test.min())\n",
    "print(X_train.mean(), X_test.mean())\n",
    "print(X_train.max(), X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.25098041,  0.        ,  0.        ,\n",
       "        0.50196081,  0.50196081,  0.50196081,  0.50196081,  0.50196081,\n",
       "        0.50196081,  0.74901962,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  0.50196081,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        0.50196081,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.50196081,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.74901962,  1.        ,  0.74901962,  0.50196081,  0.50196081,\n",
       "        0.50196081,  0.50196081,  0.74901962,  1.        ,  0.74901962,\n",
       "        0.25098041,  0.        ,  0.        ,  0.50196081,  1.        ,\n",
       "        1.        ,  0.50196081,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.50196081,  1.        ,  1.        ,  0.50196081,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.50196081,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.25098041,\n",
       "        1.        ,  1.        ,  0.50196081,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "        0.25098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.25098041,\n",
       "        1.        ,  1.        ,  1.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.74901962,  1.        ,  1.        ,\n",
       "        0.50196081,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.50196081,\n",
       "        1.        ,  1.        ,  1.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.25098041,  0.74901962,  1.        ,  1.        ,\n",
       "        0.50196081,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.74901962,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.25098041,  1.        ,  1.        ,  1.        ,  0.74901962,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.25098041,  0.50196081,  0.74901962,  1.        ,  1.        ,\n",
       "        1.        ,  0.50196081,  0.        ,  0.        ,  0.        ,\n",
       "        0.50196081,  0.25098041,  0.50196081,  0.25098041,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.25098041,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.50196081,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  0.25098041,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.25098041,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.74901962,  0.50196081,\n",
       "        0.50196081,  0.74901962,  1.        ,  1.        ,  0.50196081,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.25098041,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.50196081,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.25098041,\n",
       "        0.50196081,  0.50196081,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_idx(N, batch_size):\n",
    "    num_batches = int((N + batch_size - 1) / batch_size)\n",
    "    for i in range(num_batches):\n",
    "        start, end = i * batch_size, (i + 1) * batch_size\n",
    "        idx = slice(start, end)\n",
    "\n",
    "        yield idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "[[ 0.20202756  0.71674543  0.40515169  0.26908579]\n",
      " [ 0.99560066  0.83913098  0.79367377  0.43882338]\n",
      " [ 0.11479176  0.58573877  0.87067218  0.29892666]\n",
      " [ 0.95999942  0.11536383  0.16154735  0.5996127 ]\n",
      " [ 0.54871874  0.00921023  0.00923285  0.58679924]\n",
      " [ 0.55599164  0.55512009  0.51649735  0.41297811]\n",
      " [ 0.76172963  0.24646588  0.55570662  0.33067711]\n",
      " [ 0.39736591  0.62373095  0.1030668   0.80245893]\n",
      " [ 0.29214797  0.08662856  0.00884072  0.87150083]\n",
      " [ 0.07755372  0.43004725  0.44463644  0.44727887]\n",
      " [ 0.35454402  0.48810394  0.6811334   0.74935368]\n",
      " [ 0.79277212  0.4468647   0.00539177  0.60968035]\n",
      " [ 0.50616836  0.19045664  0.71723208  0.21069559]\n",
      " [ 0.82841097  0.576699    0.7192957   0.41491335]]\n",
      "As batches\n",
      "[[ 0.20202756  0.71674543  0.40515169  0.26908579]\n",
      " [ 0.99560066  0.83913098  0.79367377  0.43882338]\n",
      " [ 0.11479176  0.58573877  0.87067218  0.29892666]\n",
      " [ 0.95999942  0.11536383  0.16154735  0.5996127 ]]\n",
      "[[ 0.54871874  0.00921023  0.00923285  0.58679924]\n",
      " [ 0.55599164  0.55512009  0.51649735  0.41297811]\n",
      " [ 0.76172963  0.24646588  0.55570662  0.33067711]\n",
      " [ 0.39736591  0.62373095  0.1030668   0.80245893]]\n",
      "[[ 0.29214797  0.08662856  0.00884072  0.87150083]\n",
      " [ 0.07755372  0.43004725  0.44463644  0.44727887]\n",
      " [ 0.35454402  0.48810394  0.6811334   0.74935368]\n",
      " [ 0.79277212  0.4468647   0.00539177  0.60968035]]\n",
      "[[ 0.50616836  0.19045664  0.71723208  0.21069559]\n",
      " [ 0.82841097  0.576699    0.7192957   0.41491335]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.random((14, 4))\n",
    "print('Original data')\n",
    "print(X)\n",
    "print('As batches')\n",
    "for idx in get_batch_idx(X.shape[0], 4):\n",
    "    print(X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import get_all_param_values\n",
    "import os\n",
    "import pickle\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import ConcatLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import get_all_layers\n",
    "from lasagne.layers import get_all_params\n",
    "from lasagne.nonlinearities import linear, rectify, sigmoid\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne.layers import get_output\n",
    "from lasagne.layers import get_all_params\n",
    "from lasagne.updates import nesterov_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward pass for the encoder, q(z|x)\n",
    "def create_encoder_func(layers):\n",
    "    X = T.fmatrix('X')\n",
    "    X_batch = T.fmatrix('X_batch')\n",
    "\n",
    "    Z = get_output(layers['l_encoder_out'], X, deterministic=True)\n",
    "\n",
    "    encoder_func = theano.function(\n",
    "        inputs=[theano.In(X_batch)],\n",
    "        outputs=Z,\n",
    "        givens={\n",
    "            X: X_batch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return encoder_func\n",
    "\n",
    "\n",
    "# forward pass for the decoder, p(x|z)\n",
    "def create_decoder_func(layers):\n",
    "    Z = T.fmatrix('Z')\n",
    "    Z_batch = T.fmatrix('Z_batch')\n",
    "\n",
    "    X = get_output(\n",
    "        layers['l_decoder_out'],\n",
    "        inputs={\n",
    "            layers['l_encoder_out']: Z\n",
    "        },\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    decoder_func = theano.function(\n",
    "        inputs=[theano.In(Z_batch)],\n",
    "        outputs=X,\n",
    "        givens={\n",
    "            Z: Z_batch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return decoder_func\n",
    "\n",
    "\n",
    "# forward/backward (optional) pass for the encoder/decoder pair\n",
    "def create_encoder_decoder_func(layers, apply_updates=False):\n",
    "    X = T.fmatrix('X')\n",
    "    X_batch = T.fmatrix('X_batch')\n",
    "\n",
    "    X_hat = get_output(layers['l_decoder_out'], X, deterministic=False)\n",
    "\n",
    "    # reconstruction loss\n",
    "    encoder_decoder_loss = T.mean(\n",
    "        T.mean(T.sqr(X - X_hat), axis=1)\n",
    "    )\n",
    "\n",
    "    if apply_updates:\n",
    "        # all layers that participate in the forward pass should be updated\n",
    "        encoder_decoder_params = get_all_params(\n",
    "            layers['l_decoder_out'], trainable=True)\n",
    "\n",
    "        encoder_decoder_updates = nesterov_momentum(\n",
    "            encoder_decoder_loss, encoder_decoder_params, 0.01, 0.9)\n",
    "    else:\n",
    "        encoder_decoder_updates = None\n",
    "\n",
    "    encoder_decoder_func = theano.function(\n",
    "        inputs=[theano.In(X_batch)],\n",
    "        outputs=encoder_decoder_loss,\n",
    "        updates=encoder_decoder_updates,\n",
    "        givens={\n",
    "            X: X_batch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return encoder_decoder_func\n",
    "\n",
    "\n",
    "# forward/backward (optional) pass for discriminator\n",
    "def create_discriminator_func(layers, apply_updates=False):\n",
    "    X = T.fmatrix('X')\n",
    "    pz = T.fmatrix('pz')\n",
    "\n",
    "    X_batch = T.fmatrix('X_batch')\n",
    "    pz_batch = T.fmatrix('pz_batch')\n",
    "\n",
    "    # the discriminator receives samples from q(z|x) and p(z)\n",
    "    # and should predict to which distribution each sample belongs\n",
    "    discriminator_outputs = get_output(\n",
    "        layers['l_discriminator_out'],\n",
    "        inputs={\n",
    "            layers['l_prior_in']: pz,\n",
    "            layers['l_encoder_in']: X,\n",
    "        },\n",
    "        deterministic=False,\n",
    "    )\n",
    "\n",
    "    # label samples from q(z|x) as 1 and samples from p(z) as 0\n",
    "    discriminator_targets = T.vertical_stack(\n",
    "        T.ones((X_batch.shape[0], 1)),\n",
    "        T.zeros((pz_batch.shape[0], 1))\n",
    "    )\n",
    "\n",
    "    discriminator_loss = T.mean(\n",
    "        T.nnet.binary_crossentropy(\n",
    "            discriminator_outputs,\n",
    "            discriminator_targets,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if apply_updates:\n",
    "        # only layers that are part of the discriminator should be updated\n",
    "        discriminator_params = get_all_params(\n",
    "            layers['l_discriminator_out'], trainable=True, discriminator=True)\n",
    "\n",
    "        discriminator_updates = nesterov_momentum(\n",
    "            discriminator_loss, discriminator_params, 0.1, 0.0)\n",
    "    else:\n",
    "        discriminator_updates = None\n",
    "\n",
    "    discriminator_func = theano.function(\n",
    "        inputs=[\n",
    "            theano.In(X_batch),\n",
    "            theano.In(pz_batch),\n",
    "        ],\n",
    "        outputs=discriminator_loss,\n",
    "        updates=discriminator_updates,\n",
    "        givens={\n",
    "            X: X_batch,\n",
    "            pz: pz_batch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return discriminator_func\n",
    "\n",
    "\n",
    "# forward/backward (optional) pass for the generator\n",
    "# note that the generator is the same network as the encoder,\n",
    "# but updated separately\n",
    "def create_generator_func(layers, apply_updates=False):\n",
    "    X = T.fmatrix('X')\n",
    "    X_batch = T.fmatrix('X_batch')\n",
    "\n",
    "    # no need to pass an input to l_prior_in here\n",
    "    generator_outputs = get_output(\n",
    "        layers['l_encoder_out'], X, deterministic=False)\n",
    "\n",
    "    # so pass the output of the generator as the output of the concat layer\n",
    "    discriminator_outputs = get_output(\n",
    "        layers['l_discriminator_out'],\n",
    "        inputs={\n",
    "            layers['l_prior_encoder_concat']: generator_outputs,\n",
    "        },\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    # the discriminator learns to predict 1 for q(z|x),\n",
    "    # so the generator should fool it into predicting 0\n",
    "    generator_targets = T.zeros_like(X_batch.shape[0])\n",
    "\n",
    "    # so the generator needs to push the discriminator's output to 0\n",
    "    generator_loss = T.mean(\n",
    "        T.nnet.binary_crossentropy(\n",
    "            discriminator_outputs,\n",
    "            generator_targets,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if apply_updates:\n",
    "        # only layers that are part of the generator (i.e., encoder)\n",
    "        # should be updated\n",
    "        generator_params = get_all_params(\n",
    "            layers['l_discriminator_out'], trainable=True, generator=True)\n",
    "\n",
    "        generator_updates = nesterov_momentum(\n",
    "            generator_loss, generator_params, 0.1, 0.0)\n",
    "    else:\n",
    "        generator_updates = None\n",
    "\n",
    "    generator_func = theano.function(\n",
    "        inputs=[\n",
    "            theano.In(X_batch),\n",
    "        ],\n",
    "        outputs=generator_loss,\n",
    "        updates=generator_updates,\n",
    "        givens={\n",
    "            X: X_batch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return generator_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_weights(weights, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(weights, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_weights(layer, filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        src_params_list = pickle.load(f)\n",
    "\n",
    "    dst_params_list = get_all_params(layer)\n",
    "    # assign the parameter values stored on disk to the model\n",
    "    for src_params, dst_params in zip(src_params_list, dst_params_list):\n",
    "        dst_params.set_value(src_params)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    num_input = 28 * 28\n",
    "    # should really use more dimensions, but this is nice for visualization\n",
    "    num_code = 2\n",
    "    num_hidden = 1000\n",
    "\n",
    "    l_encoder_in = InputLayer((None, num_input), name='l_encoder_in')\n",
    "\n",
    "    # first layer of the encoder/generator\n",
    "    l_dense1 = DenseLayer(\n",
    "        l_encoder_in, num_units=num_hidden, nonlinearity=rectify,\n",
    "        name='l_encoder_dense1',\n",
    "    )\n",
    "    l_dense1.params[l_dense1.W].add('generator')\n",
    "    l_dense1.params[l_dense1.b].add('generator')\n",
    "\n",
    "    # second layer of the encoder/generator\n",
    "    l_dense2 = DenseLayer(\n",
    "        l_dense1, num_units=num_hidden, nonlinearity=rectify,\n",
    "        name='l_encoder_dense2',\n",
    "    )\n",
    "    l_dense2.params[l_dense2.W].add('generator')\n",
    "    l_dense2.params[l_dense2.b].add('generator')\n",
    "\n",
    "    # output of the encoder/generator: q(z|x)\n",
    "    l_encoder_out = DenseLayer(\n",
    "        l_dense2, num_units=num_code, nonlinearity=linear,\n",
    "        name='l_encoder_out',\n",
    "    )\n",
    "    l_encoder_out.params[l_encoder_out.W].add('generator')\n",
    "    l_encoder_out.params[l_encoder_out.b].add('generator')\n",
    "\n",
    "    # first layer of the decoder\n",
    "    l_decoder_in = DenseLayer(\n",
    "        l_encoder_out, num_units=num_hidden, nonlinearity=rectify,\n",
    "        name='l_decoder_dense1',\n",
    "    )\n",
    "    # second layer of the decoder\n",
    "    l_dense5 = DenseLayer(\n",
    "        l_decoder_in, num_units=num_hidden, nonlinearity=rectify,\n",
    "        name='l_decoder_dense2',\n",
    "    )\n",
    "\n",
    "    # output of the decoder: p(x|z)\n",
    "    l_decoder_out = DenseLayer(\n",
    "        l_dense5, num_units=num_input, nonlinearity=sigmoid,\n",
    "        name='l_decoder_out',\n",
    "    )\n",
    "\n",
    "    # input layer providing samples from p(z)\n",
    "    l_prior = InputLayer((None, num_code), name='l_prior_in')\n",
    "\n",
    "    # concatenate samples from q(z|x) to samples from p(z)\n",
    "    l_concat = ConcatLayer(\n",
    "        [l_encoder_out, l_prior], axis=0, name='l_prior_encoder_concat',\n",
    "    )\n",
    "\n",
    "    # first layer of the discriminator\n",
    "    l_dense6 = DenseLayer(\n",
    "        l_concat, num_units=num_hidden, nonlinearity=rectify,\n",
    "        name='l_discriminator_dense1',\n",
    "    )\n",
    "#     l_dense6.params[l_dense6.W].add('discriminator')\n",
    "#     l_dense6.params[l_dense6.b].add('discriminator')\n",
    "\n",
    "#     # second layer of the discriminator\n",
    "#     l_dense7 = DenseLayer(\n",
    "#         l_dense6, num_units=num_hidden, nonlinearity=rectify,\n",
    "#         name='l_discriminator_dense2',\n",
    "#     )\n",
    "#     l_dense7.params[l_dense7.W].add('discriminator')\n",
    "#     l_dense7.params[l_dense7.b].add('discriminator')\n",
    "\n",
    "#     # output layer of the discriminator\n",
    "#     l_discriminator_out = DenseLayer(\n",
    "#         l_dense7, num_units=1, nonlinearity=sigmoid,\n",
    "#         name='l_discriminator_out',\n",
    "#     )\n",
    "#     l_discriminator_out.params[l_discriminator_out.W].add('discriminator')\n",
    "#     l_discriminator_out.params[l_discriminator_out.b].add('discriminator')\n",
    "\n",
    "#     model_layers = get_all_layers([l_decoder_out, l_discriminator_out])\n",
    "\n",
    "#     # put all layers in a dictionary for convenience\n",
    "#     return {layer.name: layer for layer in model_layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d6ecde833041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#layer_dict =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-d481efeeefc8>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     l_dense6 = DenseLayer(\n\u001b[1;32m     75\u001b[0m         \u001b[0ml_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrectify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l_discriminator_dense1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#     l_dense6.params[l_dense6.W].add('discriminator')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/lasagne/layers/dense.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, incoming, num_units, W, b, nonlinearity, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m                  \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnonlinearities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                  **kwargs):\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDenseLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         self.nonlinearity = (nonlinearities.identity if nonlinearity is None\n\u001b[1;32m     66\u001b[0m                              else nonlinearity)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/lasagne/layers/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, incoming, name)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincoming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincoming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/lasagne/layers/base.py\u001b[0m in \u001b[0;36moutput_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/lasagne/layers/merge.py\u001b[0m in \u001b[0;36mget_output_shape_for\u001b[0;34m(self, input_shapes)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make a mutable copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "#layer_dict = \n",
    "build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_autoencoder():\n",
    "    print('building model')\n",
    "    layers = build_model()\n",
    "\n",
    "    max_epochs = 5000\n",
    "    batch_size = 128\n",
    "    weightsfile = os.path.join('weights', 'weights_train_val.pickle')\n",
    "\n",
    "    print('compiling theano functions for training')\n",
    "    print('  encoder/decoder')\n",
    "    encoder_decoder_update = create_encoder_decoder_func(\n",
    "        layers, apply_updates=True)\n",
    "    print('  discriminator')\n",
    "    discriminator_update = create_discriminator_func(\n",
    "        layers, apply_updates=True)\n",
    "    print('  generator')\n",
    "    generator_update = create_generator_func(\n",
    "        layers, apply_updates=True)\n",
    "\n",
    "    print('compiling theano functions for validation')\n",
    "    print('  encoder/decoder')\n",
    "    encoder_decoder_func = create_encoder_decoder_func(layers)\n",
    "    print('  discriminator')\n",
    "    discriminator_func = create_discriminator_func(layers)\n",
    "    print('  generator')\n",
    "    generator_func = create_generator_func(layers)\n",
    "\n",
    "    print('loading data')\n",
    "    X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            print('epoch %d' % (epoch))\n",
    "\n",
    "            # compute loss on training data and apply gradient updates\n",
    "            train_reconstruction_losses = []\n",
    "            train_discriminative_losses = []\n",
    "            train_generative_losses = []\n",
    "            for train_idx in get_batch_idx(X_train.shape[0], batch_size):\n",
    "                X_train_batch = X_train[train_idx]\n",
    "                # 1.) update the encoder/decoder to min. reconstruction loss\n",
    "                train_batch_reconstruction_loss =\\\n",
    "                    encoder_decoder_update(X_train_batch)\n",
    "\n",
    "                # sample from p(z)\n",
    "                pz_train_batch = np.random.uniform(\n",
    "                    low=-2, high=2,\n",
    "                    size=(X_train_batch.shape[0], 2)).astype(\n",
    "                        np.float32)\n",
    "\n",
    "                # 2.) update discriminator to separate q(z|x) from p(z)\n",
    "                train_batch_discriminative_loss =\\\n",
    "                    discriminator_update(X_train_batch, pz_train_batch)\n",
    "\n",
    "                # 3.)  update generator to output q(z|x) that mimic p(z)\n",
    "                train_batch_generative_loss = generator_update(X_train_batch)\n",
    "\n",
    "                train_reconstruction_losses.append(\n",
    "                    train_batch_reconstruction_loss)\n",
    "                train_discriminative_losses.append(\n",
    "                    train_batch_discriminative_loss)\n",
    "                train_generative_losses.append(\n",
    "                    train_batch_generative_loss)\n",
    "\n",
    "            # average over minibatches\n",
    "            train_reconstruction_losses_mean = np.mean(\n",
    "                train_reconstruction_losses)\n",
    "            train_discriminative_losses_mean = np.mean(\n",
    "                train_discriminative_losses)\n",
    "            train_generative_losses_mean = np.mean(\n",
    "                train_generative_losses)\n",
    "\n",
    "            print('  train: rec = %.6f, dis = %.6f, gen = %.6f' % (\n",
    "                train_reconstruction_losses_mean,\n",
    "                train_discriminative_losses_mean,\n",
    "                train_generative_losses_mean,\n",
    "            ))\n",
    "\n",
    "            # compute loss on test data\n",
    "            test_reconstruction_losses = []\n",
    "            test_discriminative_losses = []\n",
    "            test_generative_losses = []\n",
    "            for test_idx in get_batch_idx(X_test.shape[0], batch_size):\n",
    "                X_test_batch = X_test[test_idx]\n",
    "                test_batch_reconstruction_loss =\\\n",
    "                    encoder_decoder_func(X_test_batch)\n",
    "\n",
    "                # sample from p(z)\n",
    "                pz_test_batch = np.random.uniform(\n",
    "                    low=-2, high=2,\n",
    "                    size=(X_test.shape[0], 2)).astype(\n",
    "                        np.float32)\n",
    "\n",
    "                test_batch_discriminative_loss =\\\n",
    "                    discriminator_func(X_test_batch, pz_test_batch)\n",
    "\n",
    "                test_batch_generative_loss = generator_func(X_test_batch)\n",
    "\n",
    "                test_reconstruction_losses.append(\n",
    "                    test_batch_reconstruction_loss)\n",
    "                test_discriminative_losses.append(\n",
    "                    test_batch_discriminative_loss)\n",
    "                test_generative_losses.append(\n",
    "                    test_batch_generative_loss)\n",
    "\n",
    "            test_reconstruction_losses_mean = np.mean(\n",
    "                test_reconstruction_losses)\n",
    "            test_discriminative_losses_mean = np.mean(\n",
    "                test_discriminative_losses)\n",
    "            test_generative_losses_mean = np.mean(\n",
    "                test_generative_losses)\n",
    "\n",
    "            print('  test: rec = %.6f, dis = %.6f, gen = %.6f' % (\n",
    "                test_reconstruction_losses_mean,\n",
    "                test_discriminative_losses_mean,\n",
    "                test_generative_losses_mean,\n",
    "            ))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('caught ctrl-c, stopped training')\n",
    "        weights = get_all_param_values([\n",
    "            layers['l_decoder_out'],\n",
    "            layers['l_discriminator_out'],\n",
    "        ])\n",
    "        print('saving weights to %s' % (weightsfile))\n",
    "        save_weights(weights, weightsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import os\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "    dataset = pickle.load(open('data/mnist.pkl','rb'))\n",
    "    train_set_x = numpy.concatenate((dataset[0][0],dataset[1][0]),axis=0)\n",
    "    train_set_y = numpy.concatenate((dataset[0][1],dataset[1][1]),axis=0)\n",
    "    return ((train_set_x,train_set_y),(dataset[2][0],dataset[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _shared_dataset(data_xy):\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                           dtype=theano.config.floatX), borrow=True)\n",
    "    shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                           dtype='int32'), borrow=True)\n",
    "    return shared_x, shared_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist_full():\n",
    "    dataset = load_mnist_dataset()\n",
    "\n",
    "    train_set_x, train_set_y = dataset[0]\n",
    "    test_set_x, test_set_y = dataset[1]\n",
    "\n",
    "    train_set_x, train_set_y = _shared_dataset((train_set_x, train_set_y))\n",
    "    test_set_x, test_set_y = _shared_dataset((test_set_x, test_set_y))\n",
    "\n",
    "    return [(train_set_x, train_set_y), (test_set_x, test_set_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist_for_validation(n_v = 10000):\n",
    "    dataset = load_mnist_dataset()\n",
    "\n",
    "    train_set_x, train_set_y = dataset[0]\n",
    "\n",
    "    randix = numpy.random.permutation(train_set_x.shape[0])\n",
    "\n",
    "    valid_set_x = train_set_x[randix[:n_v]]\n",
    "    valid_set_y = train_set_y[randix[:n_v]]\n",
    "    train_set_x = train_set_x[randix[n_v:]]\n",
    "    train_set_y = train_set_y[randix[n_v:]]\n",
    "\n",
    "    train_set_x, train_set_y = _shared_dataset((train_set_x, train_set_y))\n",
    "    valid_set_x, valid_set_y = _shared_dataset((valid_set_x, valid_set_y))\n",
    "\n",
    "    return [(train_set_x, train_set_y), (valid_set_x, valid_set_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mnist_for_semi_sup(n_l=1000, n_v=1000):\n",
    "    dataset = load_mnist_dataset()\n",
    "\n",
    "    _train_set_x, _train_set_y = dataset[0]\n",
    "\n",
    "    rand_ind = numpy.random.permutation(_train_set_x.shape[0])\n",
    "    _train_set_x = _train_set_x[rand_ind]\n",
    "    _train_set_y = _train_set_y[rand_ind]\n",
    "\n",
    "    s_c = n_l / 10.0\n",
    "    train_set_x = numpy.zeros((n_l, 28 ** 2))\n",
    "    train_set_y = numpy.zeros(n_l)\n",
    "    for i in range(10):\n",
    "        ind = numpy.where(_train_set_y == i)[0]\n",
    "        train_set_x[i * s_c:(i + 1) * s_c, :] = _train_set_x[ind[0:s_c], :]\n",
    "        train_set_y[i * s_c:(i + 1) * s_c] = _train_set_y[ind[0:s_c]]\n",
    "        _train_set_x = numpy.delete(_train_set_x, ind[0:s_c], 0)\n",
    "        _train_set_y = numpy.delete(_train_set_y, ind[0:s_c])\n",
    "\n",
    "    print(rand_ind)\n",
    "    rand_ind = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[rand_ind]\n",
    "    train_set_y = train_set_y[rand_ind]\n",
    "    valid_set_x = _train_set_x[:n_v]\n",
    "    valid_set_y = _train_set_y[:n_v]\n",
    "    # ul_train_set_x = _train_set_x[n_v:]\n",
    "    train_set_ul_x = numpy.concatenate((train_set_x, _train_set_x[n_v:]), axis=0)\n",
    "    train_set_ul_x = train_set_ul_x[numpy.random.permutation(train_set_ul_x.shape[0])]\n",
    "    ul_train_set_y = _train_set_y[n_v:]  # dummy\n",
    "\n",
    "    train_set_x, train_set_y = _shared_dataset((train_set_x, train_set_y))\n",
    "    train_set_ul_x, ul_train_set_y = _shared_dataset((train_set_ul_x, ul_train_set_y))\n",
    "    valid_set_x, valid_set_y = _shared_dataset((valid_set_x, valid_set_y))\n",
    "\n",
    "    return [(train_set_x, train_set_y, train_set_ul_x), (valid_set_x, valid_set_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
